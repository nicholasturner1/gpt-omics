{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5783e1d7-92fe-4327-9ae0-a68883a39deb",
   "metadata": {},
   "source": [
    "# Testing logit attribution function\n",
    "Trying to compute the logit attributions of some heads and token combinations \"by hand\" and comparing those results to the full version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a763df33-ed56-4a17-bb8c-631f08b0a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from gptomics import functional as func\n",
    "from gptomics import transformersio, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e009dcc7-f01f-4f5e-bd24-f57bfcf22804",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_prompt = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8fd3d49-4998-47ce-a643-22d9473cff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a705a80-2aa5-4ed5-b3b3-01b03ea270e7",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44747684-83bd-4120-8b37-3388ace52ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.model_by_name(\"EleutherAI/gpt-neo-125M\")\n",
    "t = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00bf5cb-244e-489d-9d66-184df6957854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model.config.num_heads, m.model.config.num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802a88e-b2ae-4457-93fb-8b9ba87dc60e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5be032-7f9d-43f2-8299-30c16979ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 331 ms, total: 2.16 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%time attrs, tokens = func.logit_attribution(\"EleutherAI/gpt-neo-125M\", small_prompt, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76add01b-ac3a-4312-b865-b7379f0da287",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Picking a couple heads and token combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08266ee-e2e5-4a38-a4d0-ced57d5d584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(203834)\n",
    "head_inds = random.sample(range(m.model.config.num_heads), 2)\n",
    "layer_inds = random.sample(range(m.model.config.num_layers), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9d573d-19c9-4d33-91d2-4170a428c919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 11), (7, 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(head_inds, layer_inds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f83692-19d2-4644-86a1-5681a93349e8",
   "metadata": {},
   "source": [
    "# Extracting the hidden vectors and attention matrix at the right place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428d91f3-bc6a-4d67-b6a5-481d01d9aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_vectors = list()\n",
    "attn_matrices = list()\n",
    "\n",
    "def hook(attn_layer, hidden_states, output):\n",
    "    hidden_vectors.append(hidden_states[0])\n",
    "    attn_matrices.append(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ce9c55-b126-4c5b-b0da-7252e41cf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(m, hook, block, prompt):\n",
    "    handle = m.model.transformer.h[block].attn.attention.register_forward_hook(hook)\n",
    "    \n",
    "    input_ids = t(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = m.model(input_ids, output_attentions=True)\n",
    "        \n",
    "    handle.remove()\n",
    "    \n",
    "    return input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae0064-37bb-4967-9fcf-0c9a7db6f786",
   "metadata": {},
   "source": [
    "# Block 6, head 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc19ad4-a4af-4ace-9d72-54327336237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = run_model(m, hook, 6, small_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc863066-f026-4464-b095-af337b910c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 9), (7, 8)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "token_pairs = [(i, j) for (i, j) in itertools.product(range(10), range(10)) if i <= j]\n",
    "random.seed(3894702937)\n",
    "random.sample(token_pairs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be931e90-36df-48e0-a68c-66948ee078d8",
   "metadata": {},
   "source": [
    "## Token 9 -> 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fcf57a2c-1d00-4086-af9d-adee0eff0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 6  # block\n",
    "h = 11  # head\n",
    "src = 9  # src token\n",
    "dst = 9  # dst token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4c8bc0e-6fb0-4ceb-8f80-b1e2aaffbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant hidden vectors and attention matrix\n",
    "hvs = hidden_vectors[0]\n",
    "att = attn_matrices[0]\n",
    "att_layer = m.model.transformer.h[b].attn.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f42221d-8639-4eaf-9f5d-ba735348913f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 768]), torch.Size([1, 12, 10, 10]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvs.shape, att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b9daae8-7509-403b-bd4a-d1b7c13a7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the value vector for token 9\n",
    "v = att_layer._split_heads(\n",
    "    att_layer.v_proj(hvs), att_layer.num_heads, att_layer.head_dim\n",
    ")[0, h, src]\n",
    "# Weight by attention weight from 9 to 9\n",
    "v_ = v * att[0, h, dst, src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d84c4bf2-7eae-43e1-87df-510b6cca1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the relevant columns of the output matrix\n",
    "o = m.model.transformer.h[b].attn.attention.out_proj.weight[:, h*64:(h+1)*64]\n",
    "r = o @ v_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "81669f63-399a-4a53-bbcc-c12dcc61af75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1104)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unembed = m.model.lm_head.weight[token_ids[dst]]\n",
    "unembed @ r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aa702d2f-876c-4eca-b94e-fef06955811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1104)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch result\n",
    "attrs[b, h, src, dst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fdc0b-b945-4df3-98eb-e33fdb32bda5",
   "metadata": {},
   "source": [
    "## Token 7 -> 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98f99bc9-b1cc-4f18-b2a2-62e14bcd9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 6  # block\n",
    "h = 11  # head\n",
    "src = 7  # src token\n",
    "dst = 8  # dst token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1419800a-ea1e-48f6-95a3-b6f64babca26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0070)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant hidden vectors and attention matrix\n",
    "hvs = hidden_vectors[0]\n",
    "att = attn_matrices[0]\n",
    "\n",
    "# Form the value vector for token 9\n",
    "v = m.model.transformer.h[b].attn.attention.v_proj(hvs)[0, src, h*64:(h+1)*64]\n",
    "# Weight by attention weight from 9 to 9\n",
    "v_ = v * att[0, h, dst, src]\n",
    "\n",
    "# Fetching the relevant columns of the output matrix\n",
    "o = m.model.transformer.h[b].attn.attention.out_proj.weight[:, h*64:(h+1)*64]\n",
    "r = o @ v_\n",
    "unembed = m.model.lm_head.weight[token_ids[dst]]\n",
    "unembed @ r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bdb5dc83-b5ab-4a76-ba2c-9391a69a30c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0070)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch result\n",
    "attrs[b, h, dst, src]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc70c0f-9f4a-40ff-b33b-43fba0100dca",
   "metadata": {},
   "source": [
    "# Block 7, head 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69af807d-138e-4e66-9d42-7c191ddaf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = run_model(m, hook, 7, small_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "303828e2-7f93-4b9a-a7db-f2c710fceb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 6), (2, 8)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "token_pairs = [(i, j) for (i, j) in itertools.product(range(10), range(10)) if i <= j]\n",
    "random.seed(2397846)\n",
    "random.sample(token_pairs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb74c4-ed2a-45cf-917d-e02d0961caed",
   "metadata": {},
   "source": [
    "## Token 1 -> 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6f8eac9-07a2-45b0-9fc3-210d3b93c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 7  # block\n",
    "h = 0  # head\n",
    "src = 1  # src token\n",
    "dst = 6  # dst token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b0ebeef-0a28-4ca5-80ca-b6ff5b4cae5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1856)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant hidden vectors and attention matrix\n",
    "hvs = hidden_vectors[1]\n",
    "att = attn_matrices[1]\n",
    "\n",
    "# Form the value vector for token 9\n",
    "v = m.model.transformer.h[b].attn.attention.v_proj(hvs)[0, src, h*64:(h+1)*64]\n",
    "# Weight by attention weight from 9 to 9\n",
    "v_ = v * att[0, h, dst, src]\n",
    "\n",
    "# Fetching the relevant columns of the output matrix\n",
    "o = m.model.transformer.h[b].attn.attention.out_proj.weight[:, h*64:(h+1)*64]\n",
    "r = o @ v_\n",
    "unembed = m.model.lm_head.weight[token_ids[dst]]\n",
    "unembed @ r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d11d163f-d1b2-4f9e-aad8-2ce608740883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1856)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch result\n",
    "attrs[b, h, dst, src]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a421099-78c1-4881-b6b0-c8ea743809dc",
   "metadata": {},
   "source": [
    "## Token 2 -> 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "85fe21df-0326-4cdb-bd34-6a6a73a6b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 7  # block\n",
    "h = 0  # head\n",
    "src = 2  # src token\n",
    "dst = 8  # dst token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd6f90cb-629d-462b-aca6-38efb864adb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2034)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the relevant hidden vectors and attention matrix\n",
    "hvs = hidden_vectors[1]\n",
    "att = attn_matrices[1]\n",
    "\n",
    "# Form the value vector for token 9\n",
    "v = m.model.transformer.h[b].attn.attention.v_proj(hvs)[0, src, h*64:(h+1)*64]\n",
    "# Weight by attention weight from 9 to 9\n",
    "v_ = v * att[0, h, dst, src]\n",
    "\n",
    "# Fetching the relevant columns of the output matrix\n",
    "o = m.model.transformer.h[b].attn.attention.out_proj.weight[:, h*64:(h+1)*64]\n",
    "r = o @ v_\n",
    "unembed = m.model.lm_head.weight[token_ids[dst]]\n",
    "unembed @ r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "af7f0044-7ef3-4c5a-9c7b-110d0ffc691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2034)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch result\n",
    "attrs[b, h, dst, src]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpt-neo)",
   "language": "python",
   "name": "gpt-neo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c91b108c83889fa8a8889475d4b2552c0ba3a614a4139a7d647f43856c828e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
